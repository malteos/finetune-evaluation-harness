{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21644d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "from flair.datasets import CONLL_03\n",
    "from flair.embeddings import TransformerWordEmbeddings, TransformerDocumentEmbeddings\n",
    "from flair.models import SequenceTagger, TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    dataset_name: str = field(\n",
    "        metadata={\"help\": \"The path of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    model_name_or_path: str = field()\n",
    "\n",
    "    output_path: str = field()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da469c",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args = RunArguments(\n",
    "    dataset_name='GERMEVAL_2018_OFFENSIVE_LANGUAGE',\n",
    "    #model_name_or_path='/netscratch/mostendorff/datasets/huggingface_transformers/pytorch/gpt2',\n",
    "    model_name_or_path='/netscratch/mostendorff/datasets/huggingface_transformers/pytorch/gpt2-wechsel-german',\n",
    "    \n",
    "    output_path='./data/gpt2_conll_03',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d998064",
   "metadata": {},
   "outputs": [],
   "source": [
    "package = \"flair.datasets\"\n",
    "name = run_args.dataset_name\n",
    "\n",
    "ds_class = getattr(__import__(package, fromlist=[name]), name)\n",
    "\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus = ds_class()\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45207c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:20:11,146 Reading data from /netscratch/mostendorff/datasets/flair_cache/datasets/germeval_2018_offensive_language/coarse_grained\n",
      "2022-11-27 20:20:11,147 Train: /netscratch/mostendorff/datasets/flair_cache/datasets/germeval_2018_offensive_language/coarse_grained/train.txt\n",
      "2022-11-27 20:20:11,148 Dev: None\n",
      "2022-11-27 20:20:11,148 Test: /netscratch/mostendorff/datasets/flair_cache/datasets/germeval_2018_offensive_language/coarse_grained/test.txt\n",
      "2022-11-27 20:20:14,387 Initialized corpus /netscratch/mostendorff/datasets/flair_cache/datasets/germeval_2018_offensive_language/coarse_grained (label type name is 'class')\n",
      "Corpus: 4508 train + 501 dev + 3532 test sentences\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d20e1288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:20:24,421 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4508it [00:00, 69208.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:20:24,494 Dictionary created for label 'class' with 3 values: OTHER (seen 2991 times), OFFENSE (seen 1517 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. what label do we want to predict?\n",
    "label_type = 'class'\n",
    "\n",
    "# 3. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ea8299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:01:11,672 Could not determine the begin offset of the tokenizer for transformer model transformer-/netscratch/mostendorff/datasets/huggingface_transformers/pytorch/gpt2, assuming 0\n"
     ]
    }
   ],
   "source": [
    "# 4. initialize transformer document embeddings (many models are available)\n",
    "#document_embeddings = TransformerDocumentEmbeddings(fine_tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc58aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:24:34,581 Could not determine the begin offset of the tokenizer for transformer model transformer-/netscratch/mostendorff/datasets/huggingface_transformers/pytorch/gpt2-wechsel-german, assuming 0\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings.base import TransformerEmbedding\n",
    "\n",
    "document_embeddings = TransformerEmbedding(\n",
    "    model=run_args.model_name_or_path,\n",
    "    is_document_embedding=True,\n",
    "    fine_tune=False,\n",
    "    cls_pooling='mean',\n",
    ")\n",
    "document_embeddings.tokenizer.pad_token = document_embeddings.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd225ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type)\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. run training with fine-tuning\n",
    "fine_tune_res = trainer.fine_tune(run_args.output_path,\n",
    "      learning_rate=5.0e-5,\n",
    "      mini_batch_size=16,\n",
    "      max_epochs=1,\n",
    ")\n",
    "fine_tune_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca1f02",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93d324c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args = RunArguments(\n",
    "    dataset_name='NER_GERMAN_GERMEVAL',\n",
    "    model_name_or_path='/netscratch/mostendorff/datasets/huggingface_transformers/pytorch/gpt2-wechsel-german',\n",
    "    output_path='./data/gpt2_conll_03',\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a14d2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Jjhbal535VVz2ap4v4r_rN1UEHTdLK5P\n",
      "To: /netscratch/mostendorff/datasets/flair_cache/datasets/ner_german_germeval/train.tsv\n",
      "100%|████████████████████████████████████████████████████████████| 7.88M/7.88M [00:00<00:00, 76.1MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1u9mb7kNJHWQCWyweMDRMuTFoOHOfeBTH\n",
      "To: /netscratch/mostendorff/datasets/flair_cache/datasets/ner_german_germeval/test.tsv\n",
      "100%|████████████████████████████████████████████████████████████| 1.68M/1.68M [00:00<00:00, 35.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZfRcQThdtAR5PPRjIDtrVP7BtXSCUBbm\n",
      "To: /netscratch/mostendorff/datasets/flair_cache/datasets/ner_german_germeval/dev.tsv\n",
      "100%|██████████████████████████████████████████████████████████████| 724k/724k [00:00<00:00, 20.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:32:00,639 Reading data from /netscratch/mostendorff/datasets/flair_cache/datasets/ner_german_germeval\n",
      "2022-11-27 20:32:00,640 Train: /netscratch/mostendorff/datasets/flair_cache/datasets/ner_german_germeval/train.tsv\n",
      "2022-11-27 20:32:00,640 Dev: /netscratch/mostendorff/datasets/flair_cache/datasets/ner_german_germeval/dev.tsv\n",
      "2022-11-27 20:32:00,642 Test: /netscratch/mostendorff/datasets/flair_cache/datasets/ner_german_germeval/test.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 24000 train + 2200 dev + 5100 test sentences\n"
     ]
    }
   ],
   "source": [
    "package = \"flair.datasets\"\n",
    "name = run_args.dataset_name\n",
    "\n",
    "ds_class = getattr(__import__(package, fromlist=[name]), name)\n",
    "\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus = ds_class()\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a43f99fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:32:30,123 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24000it [00:00, 71766.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:32:30,467 Dictionary created for label 'ner' with 13 values: LOC (seen 8281 times), PER (seen 7679 times), ORG (seen 5255 times), OTH (seen 3024 times), LOCderiv (seen 2808 times), ORGpart (seen 805 times), LOCpart (seen 513 times), OTHderiv (seen 236 times), OTHpart (seen 190 times), PERpart (seen 184 times), PERderiv (seen 62 times), ORGderiv (seen 41 times)\n",
      "Dictionary with 13 tags: <unk>, LOC, PER, ORG, OTH, LOCderiv, ORGpart, LOCpart, OTHderiv, OTHpart, PERpart, PERderiv, ORGderiv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. what label do we want to predict?\n",
    "label_type = 'ner'\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a6e6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:33:21,886 Could not determine the begin offset of the tokenizer for transformer model transformer-/netscratch/mostendorff/datasets/huggingface_transformers/pytorch/gpt2-wechsel-german, assuming 0\n"
     ]
    }
   ],
   "source": [
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model=run_args.model_name_or_path,\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first_last\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ccd914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.tokenizer.pad_token = embeddings.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ae203a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:34:09,462 SequenceTagger predicts: Dictionary with 49 tags: O, S-LOC, B-LOC, E-LOC, I-LOC, S-PER, B-PER, E-PER, I-PER, S-ORG, B-ORG, E-ORG, I-ORG, S-OTH, B-OTH, E-OTH, I-OTH, S-LOCderiv, B-LOCderiv, E-LOCderiv, I-LOCderiv, S-ORGpart, B-ORGpart, E-ORGpart, I-ORGpart, S-LOCpart, B-LOCpart, E-LOCpart, I-LOCpart, S-OTHderiv, B-OTHderiv, E-OTHderiv, I-OTHderiv, S-OTHpart, B-OTHpart, E-OTHpart, I-OTHpart, S-PERpart, B-PERpart, E-PERpart, I-PERpart, S-PERderiv, B-PERderiv, E-PERderiv, I-PERderiv, S-ORGderiv, B-ORGderiv, E-ORGderiv, I-ORGderiv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type='ner',\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c580936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:35:23,433 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:35:23,435 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=1536, out_features=49, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2022-11-27 20:35:23,436 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:35:23,437 Corpus: \"Corpus: 24000 train + 2200 dev + 5100 test sentences\"\n",
      "2022-11-27 20:35:23,437 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:35:23,438 Parameters:\n",
      "2022-11-27 20:35:23,438  - learning_rate: \"0.000005\"\n",
      "2022-11-27 20:35:23,439  - mini_batch_size: \"4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:35:23,440  - patience: \"3\"\n",
      "2022-11-27 20:35:23,441  - anneal_factor: \"0.5\"\n",
      "2022-11-27 20:35:23,441  - max_epochs: \"10\"\n",
      "2022-11-27 20:35:23,442  - shuffle: \"True\"\n",
      "2022-11-27 20:35:23,443  - train_with_dev: \"False\"\n",
      "2022-11-27 20:35:23,443  - batch_growth_annealing: \"False\"\n",
      "2022-11-27 20:35:23,444 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:35:23,445 Model training base path: \"data/gpt2_conll_03\"\n",
      "2022-11-27 20:35:23,445 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:35:23,446 Device: cuda:0\n",
      "2022-11-27 20:35:23,447 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:35:23,447 Embeddings storage mode: none\n",
      "2022-11-27 20:35:23,448 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:36:37,592 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:36:37,594 Exiting from training early.\n",
      "2022-11-27 20:36:37,595 Saving model ...\n",
      "2022-11-27 20:36:38,791 Done.\n",
      "2022-11-27 20:36:38,793 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-27 20:36:38,795 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 1275/1275 [02:24<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:39:03,922 Evaluating as a multi-label problem: False\n",
      "2022-11-27 20:39:04,305 0.0041\t0.0474\t0.0076\t0.0039\n",
      "2022-11-27 20:39:04,306 \n",
      "Results:\n",
      "- F-score (micro) 0.0076\n",
      "- F-score (macro) 0.0041\n",
      "- Accuracy 0.0039\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ORG     0.0061    0.2052    0.0119      1150\n",
      "     ORGpart     0.0010    0.1279    0.0019       172\n",
      "    LOCderiv     0.0050    0.0517    0.0091       561\n",
      "         PER     0.0000    0.0000    0.0000      1639\n",
      "         LOC     0.0000    0.0000    0.0000      1706\n",
      "         OTH     0.0000    0.0000    0.0000       697\n",
      "     LOCpart     0.0171    0.0550    0.0261       109\n",
      "     PERpart     0.0000    0.0000    0.0000        44\n",
      "     OTHpart     0.0000    0.0000    0.0000        42\n",
      "    OTHderiv     0.0000    0.0000    0.0000        39\n",
      "    PERderiv     0.0000    0.0000    0.0000        11\n",
      "    ORGderiv     0.0000    0.0000    0.0000         8\n",
      "\n",
      "   micro avg     0.0041    0.0474    0.0076      6178\n",
      "   macro avg     0.0024    0.0367    0.0041      6178\n",
      "weighted avg     0.0019    0.0474    0.0036      6178\n",
      "\n",
      "2022-11-27 20:39:04,308 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.007561192758803112,\n",
       " 'dev_score_history': [],\n",
       " 'train_loss_history': [],\n",
       " 'dev_loss_history': []}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. run fine-tuning\n",
    "fine_tune_res = trainer.fine_tune(run_args.output_path,\n",
    "                  learning_rate=5.0e-6,\n",
    "                  mini_batch_size=4,\n",
    "                  #mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  )\n",
    "fine_tune_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5b9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6124790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc601d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af940b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955e519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e22a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
